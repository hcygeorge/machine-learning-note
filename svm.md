# SVM

## 說明
假設樣本有兩種分類，每個樣本是p維向量，SVM的目標就是找到一個與樣本間隔距離最大的超平面(Hyperplane)，使得超平面能切開不同分類的樣本，此時每個分類下距離超平面最近的樣本點就是支持向量(support vector)。

## 核函數
當樣本在原本空間中不是線性可分的狀態，我們可以透過將樣本映射到高維度的空間，使得樣本在該空間中變成線性可分。然而將樣本映射到高維空間會導致計算成本太高，因此SVM採用核函數(Kernel function)作為替代方案。核函數可從原本空間中直接計算樣本在高維空間的内積，如此既能降低計算成本，也不用知道映射函數的具體形式。

> 核函數用於計算樣本在高維空間的內積

以下是常見的核函數:
- Linear
- Polynomial
- Radial Basis Function(Gaussian)
- Sigmoid

核函數定義如下，對於所有的樣本，一個函數可以滿足 
$$\kappa(x_i, x_j) = <\phi(x_i), \phi(x_j)>$$
則 $\kappa(x_i, x_j)$就是一個kernel函數，其中 $\kappa$為核函數、 $\phi$為映射函數， $x_i, x_j$為任意兩個樣本點


## 優缺點
- 直接計算樣本在高維空間的内積，計算成本低且不需要知道映射函數的形式
- 如何選擇最合適的核函數是個問題
- 當數據維度越高，核函數矩陣越大，會占用更多儲存空間